{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Twoim zadaniem jest wytrenowanie klasyfikatora binarnego na podzbiorze zbioru MNIST, w którym wyróżniamy klasy (cyfry 0 i 1 mają zostać wyłączone ze zbioru):\n",
    " - Liczby pierwsze (2,3,5,7)\n",
    " - Liczby złożone (4,6,8,9)\n",
    "\n",
    "Napisz wydajną implementację modelu **regresji logistycznej** trenowanego algorytmem ***SGD z momentum***. Cały proces trenowania musisz napisać samodzielnie, w języku Python, korzystając z biblioteki numpy. Na potrzeby zadania niedozwolone jest korzystanie z gotowych implementacji optimizerów i modeli oraz bibliotek do automatycznego różniczkowania funkcji (np. Tensorflow, pytorch, autograd). \n",
    "\n",
    "Dobierz hiperparametry tak, aby uzyskać jak najlepszy wynik na zbiorze walidacyjnym. \n",
    "Wyciągnij i zapisz wnioski z przeprowadzonych eksperymentów.\n",
    "\n",
    "Zbiór MNIST dostępny jest pod linkami: \n",
    "\n",
    "(zbiór treningowy):\n",
    " - http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
    " - http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
    "\n",
    "(zbiór walidacyjny):\n",
    " - http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
    " - http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import dataset and reshape it"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.fromfile(file='train-images-idx3-ubyte',dtype=np.uint8)\n",
    "X_train = data[16:].reshape((60000,28*28)).astype(np.float128)\n",
    "\n",
    "data = np.fromfile(file='train-labels-idx1-ubyte',dtype=np.uint8)\n",
    "y_train = np.asarray(data[8:].reshape(60000))\n",
    "\n",
    "data = np.fromfile(file='t10k-images-idx3-ubyte',dtype=np.uint8)\n",
    "X_test = data[16:].reshape((10000,28*28)).astype(np.float128)\n",
    "\n",
    "data = np.fromfile(file='t10k-labels-idx1-ubyte',dtype=np.uint8)\n",
    "y_test = np.asarray(data[8:].reshape(10000))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remove (0,1) values and change labels: (2,3,5,7) = 0, (4,6,8,9) = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def drop_zero_one(X, y):\n",
    "    indexes = []\n",
    "    for i in range(len(y)):\n",
    "        if y[i] in [0,1]:\n",
    "            indexes.append(i)\n",
    "    X = np.delete(X, indexes, axis=0)\n",
    "    return X.reshape((-1,28*28)), np.delete(y, indexes)\n",
    "\n",
    "X_train_drop, y_train_drop = drop_zero_one(X_train, y_train)\n",
    "X_test_drop, y_test_drop = drop_zero_one(X_test, y_test)\n",
    "\n",
    "n_samples_train = len(y_train_drop)\n",
    "n_samples_test = len(y_test_drop)\n",
    "\n",
    "for i in range(n_samples_train):\n",
    "    y_train_drop[i]  = y_train_drop[i] in [4,6,8,9]\n",
    "\n",
    "for i in range(n_samples_test):\n",
    "    y_test_drop[i] =  y_test_drop[i] in [4,6,8,9]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Logistic Regression using Stochastic Gradient Descend with Momentum.\n",
    "\n",
    "In order to reach best results on our validation set, there is also adaptive gradient (Adagrad) improvement included."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class SGD_logistic_regression:\n",
    "\n",
    "    def __init__(self, n_epochs: int=10, learning_rate: float=0.1, momentum_const: float=0, adagrad: bool=False,\n",
    "                 save_accuracy_values: bool=False, save_cost_values: bool=False, numerical_epsilon: float=1e-7,\n",
    "                 momentum_alpha: float=-1, tolerance: float=0, shuffle_indexes=True):\n",
    "        \"\"\"\n",
    "        :param n_epochs:    sets number of epochs during the learning process\n",
    "\n",
    "        :param learning_rate:   sets learning rate of SGD\n",
    "\n",
    "        :param momentum_const:  sets the value by which the previous velocity is multiplied, within the\n",
    "                                momentum algorithm\n",
    "\n",
    "        :param momentum_alpha: sets the value by which the gradient is multiplied within momentum-velocity formula\n",
    "\n",
    "        :param adagrad:     value decides if model should use adaptive gradient, or not\n",
    "\n",
    "        :param save_accuracy_values:    value decides if model should save computed accuracy\n",
    "                                    values. If it's set as True, learning process becomes slower\n",
    "\n",
    "        :param save_cost_values:    value decides if model should save computed loss function\n",
    "                                    values. If it's set as True, learning process becomes slower\n",
    "\n",
    "        :param numerical_epsilon:   usually small value, that helps to avoid computing errors\n",
    "                                    (mostly inside exp, and log functions)\n",
    "\n",
    "        :param tolerance:   value describes tolerance of loss function. If difference between\n",
    "                            previous loss and current loss is smaller than tolerance param,\n",
    "                            the learning process stops\n",
    "        \"\"\"\n",
    "\n",
    "        self.n_epochs = n_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum_const = momentum_const\n",
    "        self.save_accuracy_value = save_accuracy_values\n",
    "        self.save_cost_value = save_cost_values\n",
    "        self.accuracy_values = []\n",
    "        self.cost_values = []\n",
    "        self.adagrad = adagrad\n",
    "        self.numerical_epsilon = numerical_epsilon\n",
    "        self.tolerance = tolerance\n",
    "        self.shuffle_indexes = shuffle_indexes\n",
    "        if momentum_alpha == -1:\n",
    "            self.momentum_alpha = 1 - self.momentum_const\n",
    "        else:\n",
    "            self.momentum_alpha = momentum_alpha\n",
    "\n",
    "    def add_bias(self, X: np.ndarray) -> np.ndarray:\n",
    "\n",
    "        \"\"\"adds intercept x0 = 1 to each instance\"\"\"\n",
    "        return np.c_[np.ones((len(X),1)), X]\n",
    "\n",
    "    def sigma(self, t: np.ndarray):\n",
    "        \"\"\"sigmoid function\"\"\"\n",
    "        return 1/(1+np.exp(-t + 1 + self.numerical_epsilon))\n",
    "\n",
    "    def normalize(self, X: np.ndarray, axis: int=-1, order: int=2) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        normalization of X parameter\n",
    "        :param X:       normalized matrix\n",
    "        \n",
    "        :param axis:    if axis is an integer, it specifies the axis of X along\n",
    "                        which to compute the vector norms\n",
    "        \n",
    "        :param order:   order of norm\n",
    "        \n",
    "        :return: X normalized with 'order' norm by 'axis' axis\n",
    "        \"\"\"\n",
    "        normed = np.atleast_1d(np.linalg.norm(X, order, axis))\n",
    "        normed[normed==0] = self.numerical_epsilon\n",
    "        return X / np.expand_dims(normed, axis)\n",
    "\n",
    "    def predict_probs(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"predicts  probability of belonging to positive class\"\"\"\n",
    "        return self.sigma(X.dot(self.theta))\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"predict if X instances belong to positive or negative class\"\"\"\n",
    "        X = self.normalize(self.add_bias(X))\n",
    "        return self.predict_probs(X) >= 0.5\n",
    "\n",
    "    def loss(self, h, y):\n",
    "        \"\"\"\n",
    "        cost function\n",
    "        :param h:   probability of belonging to positive class\n",
    "        \n",
    "        :param y:   real label\n",
    "        \n",
    "        :return:    cost which defines quality of fit\n",
    "        \"\"\"\n",
    "        return -(y.dot(np.log(h + self.numerical_epsilon)) + (1 - y).dot(np.log(1 - h + self.numerical_epsilon))).mean()\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        function computes accuracy of pointed label sets\n",
    "        :param y_true:    real labels\n",
    "        \n",
    "        :param y_pred:    predicted labels\n",
    "        \n",
    "        :return:    accuracy based on passed parameters\n",
    "        \"\"\"\n",
    "        true_pos = 0\n",
    "        true_neg = 0\n",
    "        for i in range(len(y_true)):\n",
    "            if y_true[i] == y_pred[i] == 0:\n",
    "                true_neg += 1\n",
    "            elif y_true[i] == y_pred[i] == 1:\n",
    "                true_pos += 1\n",
    "        return (true_pos + true_neg)/len(y_true)\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        \n",
    "        \"\"\"fitting function\"\"\"\n",
    "        #initializing staring values\n",
    "        X_plus_bias = self.normalize(self.add_bias(X))\n",
    "        self.theta = np.random.randn(X_plus_bias.shape[-1], 1)\n",
    "        velocity = np.zeros((X_plus_bias.shape[-1], 1))\n",
    "        squared_gradients = np.zeros((X_plus_bias.shape[-1],1))\n",
    "        train_size = len(X)\n",
    "        prev_accuracy = 0\n",
    "        best_accuracy = 0\n",
    "        best_theta = 0\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            if self.shuffle_indexes:\n",
    "                indexes = np.random.permutation(range(train_size))\n",
    "            else:\n",
    "                indexes = range(train_size)\n",
    "            for i in indexes:\n",
    "                xi = X_plus_bias[i: i+1]\n",
    "                yi = y[i]\n",
    "                y_probability =  self.sigma(xi.dot(self.theta))\n",
    "\n",
    "                #adaptive gradient improvement\n",
    "                gradient = xi.T.dot(y_probability - yi)\n",
    "                if self.adagrad:\n",
    "                    squared_gradients += gradient**2\n",
    "                    adjusted_gradient = gradient / np.sqrt(squared_gradients + self.numerical_epsilon)\n",
    "                else:\n",
    "                    adjusted_gradient = gradient\n",
    "\n",
    "                #momentum - velocity value\n",
    "                velocity = velocity * self.momentum_const + self.momentum_alpha*adjusted_gradient\n",
    "                self.theta = self.theta - self.learning_rate * velocity\n",
    "\n",
    "                #saving accuracy values\n",
    "                if self.save_accuracy_value and i < 50 and epoch < 1:\n",
    "                    y_pred = self.predict(X)\n",
    "                    self.accuracy_values.append(SGD_logistic_regression.evaluate(y, y_pred))\n",
    "\n",
    "            #after each epoch check if accuracy change is greater than the tolerance param\n",
    "            #and if current result is better that the best by now\n",
    "            y_pred = self.predict(X)\n",
    "            curr_accuracy = SGD_logistic_regression.evaluate(y, y_pred)\n",
    "            if curr_accuracy > best_accuracy:\n",
    "                best_theta = self.theta\n",
    "                best_accuracy = curr_accuracy\n",
    "\n",
    "            if abs(curr_accuracy - prev_accuracy) < self.tolerance:\n",
    "                break\n",
    "            else:\n",
    "                prev_accuracy = curr_accuracy\n",
    "\n",
    "        self.theta = best_theta"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we'll train some different models and check how they manage."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model without Momentum:  0.8976537729866836\n",
      "Accuracy of model with Momentum:  0.9155358275206088\n",
      "Accuracy of model with Momentum + Adagrad:  0.9162967660114141\n"
     ]
    }
   ],
   "source": [
    "model_no_momentum = SGD_logistic_regression(n_epochs=1, learning_rate=0.12, save_accuracy_values=True, momentum_alpha=0.3)\n",
    "model_no_momentum.fit(X_train_drop, y_train_drop)\n",
    "y_pred1 = model_no_momentum.predict(X_test_drop)\n",
    "print('Accuracy of model without Momentum: ', SGD_logistic_regression.evaluate(y_test_drop, y_pred1))\n",
    "\n",
    "model_momentum = SGD_logistic_regression(n_epochs=1, learning_rate=0.12, save_accuracy_values=True, momentum_alpha=0.3,\n",
    "                                         momentum_const=0.9)\n",
    "model_momentum.fit(X_train_drop, y_train_drop)\n",
    "y_pred2 = model_momentum.predict(X_test_drop)\n",
    "print('Accuracy of model with Momentum: ', SGD_logistic_regression.evaluate(y_test_drop, y_pred2))\n",
    "\n",
    "model_momentum_adagrad = SGD_logistic_regression(n_epochs=1, learning_rate=0.12, save_accuracy_values=True, momentum_alpha=0.3,\n",
    "                                                 momentum_const=0.9, adagrad=True)\n",
    "model_momentum_adagrad.fit(X_train_drop, y_train_drop)\n",
    "y_pred3 = model_momentum_adagrad.predict(X_test_drop)\n",
    "print('Accuracy of model with Momentum + Adagrad: ', SGD_logistic_regression.evaluate(y_test_drop, y_pred3))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Let's take a look at the differences between cost function of each model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(len(model_no_momentum.accuracy_values)), model_no_momentum.accuracy_values,\n",
    "         'g-', label = 'Accuracy without momentum')\n",
    "plt.plot(range(len(model_momentum.accuracy_values)), model_momentum.accuracy_values,\n",
    "         'r-', label = 'Accuracy with momentum')\n",
    "plt.plot(range(len(model_momentum_adagrad.accuracy_values)), model_momentum_adagrad.accuracy_values,\n",
    "         'b-', label = 'Accuracy with Momentum + Adagrad')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "It is clearly seen that the curves of the momentum model fall earlier\n",
    "This is due to the accumulation of momentum in the directions most often indicated by the gradient\n",
    "\n",
    "The plot above, shows that at the beginning, there is a difference in Momentum/no-Momentum models accuracy.\n",
    "Models which use Momentum algorithm, tend to get closer to convergence point, quicker than the one without momentum.\n",
    "\n",
    "The best result is reached by the one with adaptive gradient modification.\n",
    "A blue line is clearly visible at the top of our plot, almost from the beginning.\n",
    "\n",
    "Now I'll compare our model with a Logistic Regression model from Scikit-Learn library\n",
    "In order to quickly achieve high accuracy, we set the momentum_alpha parameter to 1, so that the formula\n",
    "for the velocity in the momentum algorithm takes the form velocity = velocity0 * momentum_const + 1*gradient"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timing:  0:00:04.564559\n",
      "Our model accuracy:  0.9189600507292327\n",
      "timing:  0:00:04.611633\n",
      "Scikit Learn Logistic Regression model accuracy:  0.9222574508560558\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "best_model = SGD_logistic_regression(n_epochs=1, learning_rate=0.12, momentum_const=0.90, adagrad=True, momentum_alpha=1)\n",
    "best_model.fit(X_train_drop, y_train_drop)\n",
    "pred = best_model.predict(X_test_drop)\n",
    "print('timing: ', datetime.datetime.now() - now)\n",
    "print('Our model accuracy: ', model_no_momentum.evaluate(y_test_drop, pred))\n",
    "\n",
    "#Our models raining process includes normalization\n",
    "normalizer = Normalizer()\n",
    "X_train_drop = normalizer.fit_transform(X_train_drop, y_train_drop)\n",
    "X_test_drop = normalizer.fit_transform(X_test_drop, y_test_drop)\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "mod = LogisticRegression(max_iter=100)\n",
    "mod.fit(X_train_drop, y_train_drop)\n",
    "pred2 = mod.predict(X_test_drop)\n",
    "print('timing: ', datetime.datetime.now() - now)\n",
    "print('Scikit Learn Logistic Regression model accuracy: ', model_no_momentum.evaluate(y_test_drop, pred2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model confusion matrix:\n",
      " [[3577  385]\n",
      " [ 254 3669]]\n",
      "Sklearn model confusion matrix:\n",
      " [[3642  320]\n",
      " [ 293 3630]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "matrix = confusion_matrix(y_test_drop, pred)\n",
    "print(\"Our model confusion matrix:\\n\", matrix)\n",
    "matrix = confusion_matrix(y_test_drop, pred2)\n",
    "print(\"Sklearn model confusion matrix:\\n\", matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The confusion matrices look alike, so there are no surprises in our predictions.\n",
    "\n",
    "# Results\n",
    "Our model reaches similar accuracy with only 1 epoch of training process.\n",
    "Our timing is also close to the scikit-learn one.\n",
    "\n",
    "Now we'll set n_epochs param as a some bigger number to check if we can get better accuracy."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timing:  0:01:32.904764\n",
      "Our model accuracy:  0.9225110970196576\n"
     ]
    }
   ],
   "source": [
    "now = datetime.datetime.now()\n",
    "best_model = SGD_logistic_regression(n_epochs = 50, learning_rate=0.12, momentum_const=0.9, adagrad=True)\n",
    "best_model.fit(X_train_drop, y_train_drop)\n",
    "pred = best_model.predict(X_test_drop)\n",
    "print('timing: ', datetime.datetime.now() - now)\n",
    "print('Our model accuracy: ', model_no_momentum.evaluate(y_test_drop, pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can tell that, our classifier is close to it's limit, already with 1 epoch of running.\n",
    "Using Momentum improvement, we are able to minimize the shaking caused by randomly chosen samples.\n",
    "Additionally, adaptive gradient helps us to manipulate the learning rate, so infrequent features got extra boost.\n",
    "This is extremely useful in sparse data sets, and our MNIST number-photos belong to this kind.(lots of empty/white pixels)\n",
    "\n",
    "When completing the task I supported myself, with the knowledge read in the following publications,\n",
    "https://ruder.io/optimizing-gradient-descent/\n",
    "https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d\n",
    "\n",
    "and on my trials, ups and downs :) "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}